{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "from multiprocessing import Pool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper methods for creating, uploading data and training an object detection model.\n",
    "def create_new_model(base_url, auth_key, categories):\n",
    "    \"\"\"\n",
    "    function to create a new model for training\n",
    "    \n",
    "    Args:\n",
    "    base_url: url to nanonets endpoint which will decide what type of model to create\n",
    "    auth_key: authentication key provided by https://app.nanonets.com/#/keys\n",
    "    categories: List of labels you want to predict/ detect\n",
    "    \n",
    "    return:\n",
    "    model_id: a unique reference to new created model\n",
    "    \"\"\"\n",
    "       \n",
    "    payload = json.dumps({\"categories\" : categories})\n",
    "    headers = {\n",
    "        'Content-Type': \"application/json\",\n",
    "        }\n",
    "\n",
    "    response = requests.request(\n",
    "        \"POST\",\n",
    "        base_url,\n",
    "        headers=headers,\n",
    "        auth=requests.auth.HTTPBasicAuth(auth_key, ''),\n",
    "        data=payload,\n",
    "    )\n",
    "\n",
    "    result = json.loads(response.text)\n",
    "    print(\"Model Information: \", result)\n",
    "    model_id, model_type, categories = (result[\"model_id\"], result[\"model_type\"], result[\"categories\"])\n",
    "    return model_id\n",
    "\n",
    "def get_model_info(base_url, auth_key, model_id):\n",
    "    \"\"\"\n",
    "    function to get/ print information about model at any time\n",
    "    \n",
    "    Args:\n",
    "    base_url: url to nanonets endpoint which will decide what type of model to create\n",
    "    auth_key: authentication key provided by https://app.nanonets.com/#/keys\n",
    "    model_id: unique model_id generated at model creation time\n",
    "    \"\"\"\n",
    "    print('%s%s'%(base_url, model_id))\n",
    "    response = requests.request(\n",
    "        'GET',\n",
    "        '%s%s'%(base_url, model_id),\n",
    "        auth=requests.auth.HTTPBasicAuth(auth_key, '')\n",
    "    )\n",
    "    print(response.text)\n",
    "    result = json.loads(response.text)\n",
    "    model_id, model_type, categories, state = (result[\"model_id\"], result[\"model_type\"], result[\"categories\"], result[\"state\"])\n",
    "    return model_id, model_type, categories, state\n",
    "\n",
    "\n",
    "def generate_upload_data(image_file, annotation_info, model_id):\n",
    "    \"\"\"\n",
    "    function to translate image and annotation info into format suitable for upload\n",
    "    \n",
    "    Args:\n",
    "    image_file[str]: full path to where the image is located\n",
    "    annotation_info[str]: json formatted string of the object annotations in the image\n",
    "        eg. '[{\"name\": \"object_1\", \"bndbox\": {\"xmin\": 50, \"ymin\": 50, \"xmax\": 100, \"ymax\": 100}}, ...]'\n",
    "    model_id[str]: model id for which data needs to be uploaded\n",
    "    \n",
    "    Returns:\n",
    "    data[Dict[str, Any]]: data that can be passed onto to the upload data method \n",
    "    \"\"\"\n",
    "    data = {\n",
    "        'file' : open(image_file, 'rb'),\n",
    "        'data' :('', '[{\"filename\":\"%s\", \"object\": %s}]' % (image_file.rsplit('/', 1)[1], annotation_info)),\n",
    "        'modelId' :('', '%s'% model_id),\n",
    "    }\n",
    "    return data\n",
    "\n",
    "\n",
    "def upload_data(base_url, model_id, auth_key, data):\n",
    "    \"\"\"\n",
    "    function to upload data for a model that has been created\n",
    "    \n",
    "    Args:\n",
    "    base_url[str]: nanonets endpoint to which the model upload request will be sent\n",
    "        eg. https://app.nanonets.com/api/v2/ObjectDetection/Model/\n",
    "    model_id[str]: model id of the model for which data is being uploaded generated by calling the create_model method\n",
    "    auth_key[str]: authentication key provided by https://app.nanonets.com/#/keys\n",
    "    data[Dict[str, Any]]: dictionary recieved from the generate_upload_data method\n",
    "    \"\"\"\n",
    "    response = requests.post(\n",
    "        '%s%s/UploadFile/'% (base_url, model_id),\n",
    "        auth=requests.auth.HTTPBasicAuth(auth_key, ''),\n",
    "        files=data,\n",
    "    )\n",
    "    print(response.text)\n",
    "    \n",
    "    \n",
    "def train_model(base_url, auth_key, model_id):\n",
    "\n",
    "    headers = {'authorization': 'Basic %s'%auth_key}\n",
    "    querystring = {'modelId': model_id}\n",
    "    response = requests.request(\n",
    "        'POST',\n",
    "        '%s%s/Train/'%(base_url, model_id),\n",
    "        headers=headers,\n",
    "        auth=requests.auth.HTTPBasicAuth(AUTH_KEY, ''),\n",
    "        params=querystring,\n",
    "    )\n",
    "    print(\"training started .... \")\n",
    "    print(json.loads(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together\n",
    "# Object Detection for Millenium Falcon "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONSTANTS\n",
    "Some universal constants that we will need for creating new models, uploading data and launching training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL_URL = \"https://app.nanonets.com/api/v2/ObjectDetection/Model/\"\n",
    "CATEGORIES = ['TieFighter', 'MillenniumFalcon']\n",
    "AUTH_KEY = \"<AUTH_KEY_FROM_NANONETS_APP>\" ## can be foung https://app.nanonets.com/#/keys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Data directly from github?\n",
    "If you already have a copy of the images and annotations required, you can set the following variable to False and update the image_directory and annotation_directory with the values to the local paths.\n",
    "Else you can directly use the image and annotations available on github to train a new object detection model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_github_data = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the Data\n",
    "If you already have a local copy of the data available you can skip the next few cells and directly update the image_directory and annotation_directory values with the location of the images and annotations respectively.\n",
    "\n",
    "If you do not have a local copy of the data, the next 3 cells, will download the images and annotations from the object-detection-sample github repo store them in a local directory which will then be used to launch an object detection job.\n",
    "\n",
    "PS: The directories will be deleted as soon as the job has been launched.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "git_repo_url = \"https://github.com/NanoNets/object-detection-sample-python/tree/master\"\n",
    "git_repo_images = os.path.join(git_repo_url, \"images\")\n",
    "git_repo_annotations = os.path.join(git_repo_url, \"annotations/json/\")\n",
    "raw_github_url = 'https://raw.githubusercontent.com/NanoNets/object-detection-sample-python/master/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download images to temp folder\n",
    "def download_file_to(source_url, destination_location):\n",
    "    f = open(destination_location, 'wb')\n",
    "    f.write(requests.get(source_url).content)\n",
    "    f.close()\n",
    "\n",
    "def download_file(file_name, base_source_url, base_destination_location):\n",
    "    source_url = os.path.join(base_source_url, file_name)\n",
    "    destination_location = os.path.join(base_destination_location, file_name)\n",
    "    download_file_to(source_url, destination_location)\n",
    "    \n",
    "\n",
    "def download_file_multiprocess(download_information):\n",
    "    file_name, base_source_url, base_destination_location = download_information\n",
    "    download_file(file_name, base_source_url, base_destination_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download annotations to temp folder\n",
    "if use_github_data:\n",
    "    p = Pool(5)\n",
    "    page = requests.get(git_repo_images)\n",
    "    image_files = set()\n",
    "    pattern = re.compile(\"(videoplayback[\\d]*\\.jpg)\")\n",
    "    for line in page.iter_lines():\n",
    "        if not line:\n",
    "            continue\n",
    "        matches = pattern.findall(str(line))    \n",
    "        if not matches:\n",
    "            continue\n",
    "        for i in matches:\n",
    "            image_files.add(i)\n",
    "\n",
    "    github_images_url = os.path.join(raw_github_url, \"images\")\n",
    "    temp_images_folder = tempfile.mkdtemp(suffix=\"images\")\n",
    "    p.map(\n",
    "        download_file_multiprocess,\n",
    "        [(file_name, github_images_url, temp_images_folder) for file_name in image_files]\n",
    "    )\n",
    "    \n",
    "    \n",
    "    page = requests.get(git_repo_annotations)\n",
    "    annotation_files = set()\n",
    "    pattern = re.compile(\"(videoplayback[\\d]*\\.json)\")\n",
    "    for line in page.iter_lines():\n",
    "        if not line:\n",
    "            continue\n",
    "        matches = pattern.findall(str(line))    \n",
    "        if not matches:\n",
    "            continue\n",
    "        for i in matches:\n",
    "            annotation_files.add(i)\n",
    "\n",
    "    github_annotations_url = os.path.join(raw_github_url, \"annotations/json/\")\n",
    "    temp_annotations_folder = tempfile.mkdtemp(suffix=\"annotations\")\n",
    "    p.map(\n",
    "        download_file_multiprocess,\n",
    "        [(file_name, github_annotations_url, temp_annotations_folder) for file_name in annotation_files]\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_model\n",
    "model_id = create_new_model(base_url=BASE_MODEL_URL, auth_key=AUTH_KEY, categories=CATEGORIES)\n",
    "\n",
    "print(\"New model created: \", model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate and upload_data\n",
    "# change current working directory to location where object-detection-sample-python repo is cloned\n",
    "image_directory = temp_images_folder  # REPLACE WITH LOCAL FOLDER IF ALREADY EXISTS\n",
    "annotation_directory = temp_annotations_folder  # REPLACE WITH LOCAL FOLDER IF ALREADY EXISTS\n",
    "annotation_files = os.listdir(annotation_directory)\n",
    "\n",
    "\n",
    "# get the image and annotation info in format easy to upload\n",
    "image_and_annotations = []\n",
    "\n",
    "for annotation_file in annotation_files:\n",
    "    with open(os.path.join(annotation_directory, annotation_file)) as f:\n",
    "        # check corresponding image file exists\n",
    "        image_path = os.path.join(image_directory, os.path.basename(annotation_file).replace(\"json\", \"jpg\"))\n",
    "        if not os.path.exists(image_path):\n",
    "            # skipping annotation as image does not exist\n",
    "            continue\n",
    "        annotation_info = f.readline().strip()\n",
    "        image_and_annotations.append((image_path, annotation_info, model_id))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload image and annotation using the previously built helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This process will however be very slow as it will upload each image and annotation information serially.\n",
    "# # We can speed this process up significantly by using the python Multiprocessing module.\n",
    "\n",
    "# total_images = len(image_and_annotations)\n",
    "# for i, (image_file, annotation_info, model_id) in enumerate(image_and_annotations):\n",
    "#     print(\"Processing Image %d / %d \" % (i, total_images))\n",
    "#     data = generate_upload_data(image_file, annotation_info, model_id)\n",
    "#     upload_data(BASE_MODEL_URL, model_id, AUTH_KEY, data)\n",
    "#     i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload the data for the model to be trained, using multiprocessig to make data upload faster.\n",
    "def upload_data_multiprocessing(image_and_annotation):\n",
    "    image_file, annotation_info, model_id = image_and_annotation\n",
    "    data = generate_upload_data(image_file, annotation_info, model_id)\n",
    "    upload_data(BASE_MODEL_URL, model_id, AUTH_KEY, data)\n",
    "    \n",
    "\n",
    "p = Pool(4)\n",
    "p.map(upload_data_multiprocessing, image_and_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model info \n",
    "get_model_info(BASE_MODEL_URL, AUTH_KEY, model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# launch training for model once all data has been uploaded\n",
    "train_model(BASE_MODEL_URL, AUTH_KEY, model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_model_info(BASE_MODEL_URL, AUTH_KEY, model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete temp folders\n",
    "if use_github_data:\n",
    "    # delete temp images folder\n",
    "    shutil.rmtree(temp_images_folder)\n",
    "    # delete temp annotations folder\n",
    "    shutil.rmtree(temp_annotations_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
